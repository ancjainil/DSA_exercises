{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\harka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\harka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg \n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd  \n",
    "import numpy as np \n",
    "import sklearn\n",
    "import random \n",
    "import re\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def generate_data(books):\n",
    "    content= [] \n",
    "    \n",
    "    for book in books: \n",
    "        a = gutenberg.raw(book)\n",
    "        content.append(a)\n",
    "\n",
    "    return content\n",
    "\n",
    "books = ['austen-emma.txt', 'shakespeare-hamlet.txt', 'melville-moby_dick.txt', 'bible-kjv.txt', 'blake-poems.txt']\n",
    "\n",
    "contents = generate_data(books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solution(): \n",
    "    def __init__(self):\n",
    "        self.content = contents\n",
    "        self.dataset = []\n",
    "\n",
    "    def tokenizer(self, contents):\n",
    "        for content in contents:\n",
    "            tokens = nltk.word_tokenize(content)\n",
    "            stop_words = set(stopwords.words('english'))\n",
    "            filtered_tokens = [word for word in tokens if \n",
    "                               (word.isalnum() and word.lower() not in stop_words)]\n",
    "            dataset = self.dataset.append(filtered_tokens)\n",
    "        return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "generator = Solution()\n",
    "print(generator.tokenizer(contents))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[102], line 65\u001b[0m\n\u001b[0;32m     61\u001b[0m generator\u001b[38;5;241m.\u001b[39mgenerate_dataset()\n\u001b[0;32m     63\u001b[0m train_set, test_set \u001b[38;5;241m=\u001b[39m generator\u001b[38;5;241m.\u001b[39msplit_dataset(train_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m)\n\u001b[1;32m---> 65\u001b[0m \u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m()\n\u001b[0;32m     67\u001b[0m generator\u001b[38;5;241m.\u001b[39msave_dataset(train_set, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_dataset.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     68\u001b[0m generator\u001b[38;5;241m.\u001b[39msave_dataset(test_set, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_dataset.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "class TextDatasetGenerator:\n",
    "    def __init__(self, books):\n",
    "        self.books = books\n",
    "        self.dataset = []\n",
    "\n",
    "    def remove_stopwords_and_punctuation(self, tokens):\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        filtered_tokens = [word for word in tokens if (word.isalnum() and word.lower() not in stop_words)]\n",
    "        return filtered_tokens\n",
    "\n",
    "    def create_records(self, content, label):\n",
    "        tokens = nltk.word_tokenize(content)\n",
    "        cleaned_tokens = self.remove_stopwords_and_punctuation(tokens)\n",
    "        records = [cleaned_tokens[i:i + 100] for i in range(0, len(cleaned_tokens), 100)]\n",
    "\n",
    "        for i, record in enumerate(records):\n",
    "            record_label = f'{label}_{i}'\n",
    "            self.dataset.append((record_label, record))\n",
    "\n",
    "    def generate_dataset(self):\n",
    "        for i, book_content in enumerate(self.books):\n",
    "            # Label each book with a unique identifier (e.g., a, b, c...)\n",
    "            label = chr(97 + i)\n",
    "            self.create_records(book_content, label)\n",
    "\n",
    "        # Shuffle the dataset\n",
    "        random.shuffle(self.dataset)\n",
    "\n",
    "    def split_dataset(self, train_ratio=0.8):\n",
    "        # Split the dataset into training and testing sets\n",
    "        train_size = int(train_ratio * len(self.dataset))\n",
    "        train_set = self.dataset[:train_size]\n",
    "        test_set = self.dataset[train_size:]\n",
    "        return train_set, test_set\n",
    "\n",
    "    def create_dataframe(self, dataset):\n",
    "        df = pd.DataFrame(dataset, columns=['Label', 'Record'])\n",
    "        return df\n",
    "\n",
    "    def save_dataset(self, dataset, filename):\n",
    "        df = self.create_dataframe(dataset)\n",
    "        df.to_csv(filename, index=False)\n",
    "\n",
    "generator = TextDatasetGenerator(contents)\n",
    "generator.generate_dataset()\n",
    "\n",
    "train_set, test_set = generator.split_dataset(train_ratio=0.8)\n",
    "\n",
    "\n",
    "\n",
    "generator.save_dataset(train_set, 'train_dataset.csv')\n",
    "generator.save_dataset(test_set, 'test_dataset.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[103], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "train_set.shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
