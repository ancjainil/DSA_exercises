{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\harka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\harka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg \n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd  \n",
    "import numpy as np \n",
    "import random \n",
    "import re\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def generate_data(books):\n",
    "    content= [] \n",
    "    \n",
    "    for book in books: \n",
    "        a = gutenberg.raw(book)\n",
    "        content.append(a)\n",
    "\n",
    "    return content\n",
    "\n",
    "books = ['austen-emma.txt', 'shakespeare-hamlet.txt', 'melville-moby_dick.txt', 'bible-kjv.txt']\n",
    "\n",
    "contents = generate_data(books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Label                                          Partition      Book\n",
      "0       a  Emma by Jane Austen 1816 VOLUME I CHAPTER I Em...  Author 1\n",
      "1       a  her caresses and her place had been supplied b...  Author 1\n",
      "2       a  together as friend and friend very mutually at...  Author 1\n",
      "3       a  of any disagreeable consciousness Miss Taylor ...  Author 1\n",
      "4       a  Weston was a man of unexceptionable character ...  Author 1\n",
      "..    ...                                                ...       ...\n",
      "795     d  Now therefore my son obey my voice according t...  Author 4\n",
      "796     d  and I shall seem to him as a deceiver and I sh...  Author 4\n",
      "797     d  put the skins of the kids of the goats upon hi...  Author 4\n",
      "798     d  bless me 27 20 And Isaac said unto his son How...  Author 4\n",
      "799     d  his hands were hairy as his brother Esau s han...  Author 4\n",
      "\n",
      "[800 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import re \n",
    "import numpy as np \n",
    "import random\n",
    "\n",
    "\n",
    "def processing(books, num_partitions, words_per_partition ):\n",
    "    def get_data(text, num_partitions, words_per_partition): \n",
    "        \"\"\"\n",
    "            Dividing the strings in the partitions\n",
    "        \"\"\"\n",
    "        words = re.findall(r'\\b\\w+\\b', text)\n",
    "        total_words = len(words)\n",
    "        partitions = [] \n",
    "\n",
    "        for i in range(num_partitions):\n",
    "            start_index = i * words_per_partition\n",
    "            end_index = (i + 1) * words_per_partition\n",
    "            partition_words = words[start_index:end_index]\n",
    "            partition_text = ' '.join(partition_words)\n",
    "            partitions.append(partition_text)\n",
    "        \n",
    "        return partitions\n",
    "\n",
    "    output_df = [] \n",
    "    \"\"\"\n",
    "        Looping through the books\n",
    "    \"\"\"\n",
    "    for i, book in enumerate(books):\n",
    "        #with open(book, 'r', encoding='utf-8') as file:\n",
    "        book_text = book\n",
    "        \n",
    "        partitions = get_data(book_text, num_partitions, words_per_partition)\n",
    "        labels = [f'{chr(ord(\"a\") + i)}' for _ in range(len(partitions))]\n",
    "\n",
    "        data = {\"Label\": labels, 'Partition': partitions, \"Book\": [f'Author {i+1}' for _ in range(len(partitions))]}\n",
    "        df = pd.DataFrame(data)\n",
    "        df1 = df.apply(np.random.permutation, axis=1)  \n",
    "        output_df.append(df)\n",
    "\n",
    "    final_df = pd.concat(output_df, ignore_index=True)\n",
    "    return final_df\n",
    "\n",
    "#### Output \n",
    "books = contents #['./book1.txt', './book2.txt', './book3.txt']\n",
    "output = processing(books, 200, 100)\n",
    "print(output)\n",
    "output.to_csv('output_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df = pd.read_csv('D:\\App Dev\\Python_workspace\\output_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with CountVectorizer: 0.0\n",
      "Accuracy with TF-IDF Vectorizer: 0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(df['Partition'], df['Label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocessing: Remove stop words and garbage characters\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Remove stop words\n",
    "    text = ' '.join([word for word in text.split() if word.lower() not in stop_words])\n",
    "    return text\n",
    "\n",
    "# Apply preprocessing to training data\n",
    "train_data = train_data.apply(preprocess_text)\n",
    "\n",
    "# Create a pipeline with CountVectorizer and Multinomial Naive Bayes\n",
    "count_vectorizer_model = make_pipeline(CountVectorizer(), MultinomialNB())\n",
    "\n",
    "# Train the model with CountVectorizer\n",
    "count_vectorizer_model.fit(train_data, train_labels)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions_count_vectorizer = count_vectorizer_model.predict(test_data)\n",
    "\n",
    "# Evaluate accuracy with CountVectorizer\n",
    "accuracy_count_vectorizer = accuracy_score(test_labels, predictions_count_vectorizer)\n",
    "print(f\"Accuracy with CountVectorizer: {accuracy_count_vectorizer}\")\n",
    "\n",
    "# Create a pipeline with TF-IDF Vectorizer and Multinomial Naive Bayes\n",
    "tfidf_vectorizer_model = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
    "\n",
    "# Train the model with TF-IDF Vectorizer\n",
    "tfidf_vectorizer_model.fit(train_data, train_labels)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions_tfidf_vectorizer = tfidf_vectorizer_model.predict(test_data)\n",
    "\n",
    "# Evaluate accuracy with TF-IDF Vectorizer\n",
    "accuracy_tfidf_vectorizer = accuracy_score(test_labels, predictions_tfidf_vectorizer)\n",
    "print(f\"Accuracy with TF-IDF Vectorizer: {accuracy_tfidf_vectorizer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
