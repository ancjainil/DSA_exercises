{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import gutenberg\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('gutenberg')\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "class GutenbergPartitioner:\n",
    "    def __init__(self):\n",
    "        # Select books from a specific genre\n",
    "        self.all_books = gutenberg.fileids()\n",
    "        self.selected_books = random.sample(self.all_books, 6)  # Choose 6 books randomly\n",
    "        self.book_data = [(book, gutenberg.raw(book)) for book in self.selected_books]\n",
    "\n",
    "    def get_author(self, book_text):\n",
    "        # Extract author information from book text\n",
    "        # This is a simplistic approach and may not work for all books\n",
    "        lines = book_text.split('\\n')\n",
    "        for line in lines:\n",
    "            if 'by' in line.lower():\n",
    "                author_index = line.lower().index('by')\n",
    "                author = line[author_index + 2:].strip()\n",
    "                # Remove any year like 1909] from author's name\n",
    "                author = ''.join([i for i in author if not i.isdigit() and i != ']'])\n",
    "                return author\n",
    "        return 'Unknown Author'\n",
    "\n",
    "    def split_into_partitions(self, num_partitions=200, partition_size=100):\n",
    "        partitions = []\n",
    "        for book, data in self.book_data:\n",
    "            author = self.get_author(data)\n",
    "            words = word_tokenize(data)\n",
    "            for _ in range(num_partitions):\n",
    "                start_index = random.randint(0, len(words) - partition_size)\n",
    "                partition = ' '.join(words[start_index:start_index + partition_size])\n",
    "                partitions.append((book, author, partition))\n",
    "\n",
    "        # Shuffle the order of partitions\n",
    "        random.shuffle(partitions)\n",
    "\n",
    "        # Label the partitions with alphabetical labels\n",
    "        labels = {book: chr(ord('a') + i) for i, book in enumerate(self.selected_books)}\n",
    "\n",
    "        # Add the labels to the partitions\n",
    "        labeled_partitions = [(labels[book], author, partition) for book, author, partition in partitions]\n",
    "        return labeled_partitions\n",
    "\n",
    "    def save_to_csv(self, labeled_partitions, output_filename='random_partitions.csv'):\n",
    "        # Serialize the data using Pandas\n",
    "        df = pd.DataFrame(labeled_partitions, columns=['Book', 'Author', 'Partition'])\n",
    "        df.to_csv(output_filename, index=False)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize, remove stopwords, and non-alphabetic characters\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    tokens = [word.lower() for word in tokens if word.isalpha() and word.lower() not in stopwords]\n",
    "\n",
    "    # Perform stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    # Perform lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token, pos=get_wordnet_pos(token)) for token in stemmed_tokens]\n",
    "\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    # Map POS tag to first character lemmatize() accepts\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def train_model(classifier, vectorizer):\n",
    "    partitioner = GutenbergPartitioner()\n",
    "    labeled_partitions = partitioner.split_into_partitions()\n",
    "    partitioner.save_to_csv(labeled_partitions)\n",
    "\n",
    "    # Load labeled partitions from CSV\n",
    "    df = pd.read_csv('random_partitions.csv')\n",
    "\n",
    "    # Preprocess the data\n",
    "    df['Processed_Partition'] = df['Partition'].apply(preprocess_text)\n",
    "\n",
    "    # Feature extraction using TF-IDF\n",
    "    features = vectorizer.fit_transform(df['Processed_Partition']).toarray()\n",
    "    labels = df['Book']\n",
    "    authors = df['Author']  # Include authors as a separate column\n",
    "\n",
    "    # Train a machine learning model\n",
    "    classifier.fit(features, authors)  # Change labels to authors\n",
    "\n",
    "    return classifier, vectorizer, authors\n",
    "\n",
    "def evaluate_model(classifier, vectorizer, authors):\n",
    "    # Load labeled partitions from CSV\n",
    "    df = pd.read_csv('random_partitions.csv')\n",
    "\n",
    "    # Preprocess the data\n",
    "    df['Processed_Partition'] = df['Partition'].apply(preprocess_text)\n",
    "\n",
    "    # Feature extraction using TF-IDF\n",
    "    features = vectorizer.transform(df['Processed_Partition']).toarray()\n",
    "    labels = df['Book']\n",
    "\n",
    "    # Perform k-fold cross-validation\n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "    accuracy_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "    roc_auc_scores = []\n",
    "\n",
    "    for train_index, test_index in kf.split(features):\n",
    "        X_train, X_test = features[train_index], features[test_index]\n",
    "        y_train, y_test = authors[train_index], authors[test_index]  # Change labels to authors\n",
    "\n",
    "        classifier.fit(X_train, y_train)\n",
    "        y_pred = classifier.predict(X_test)\n",
    "\n",
    "        accuracy_scores.append(accuracy_score(y_test, y_pred))\n",
    "        precision_scores.append(precision_score(y_test, y_pred, average='weighted'))\n",
    "        recall_scores.append(recall_score(y_test, y_pred, average='weighted'))\n",
    "        f1_scores.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "        roc_auc_scores.append(roc_auc_score(y_test, classifier.predict_proba(X_test), multi_class='ovr'))\n",
    "\n",
    "    return {\n",
    "        'Accuracy': np.mean(accuracy_scores),\n",
    "        'Precision': np.mean(precision_scores),\n",
    "        'Recall': np.mean(recall_scores),\n",
    "        'F1-score': np.mean(f1_scores),\n",
    "        'ROC-AUC': np.mean(roc_auc_scores)\n",
    "    }\n",
    "\n",
    "def print_results_subset(df, test_labels, predictions, authors, start_idx, end_idx):\n",
    "    print(f\"\\nSubset of results:\")\n",
    "    print(\"{:<10} {:<30} {:<30} {:<30}\".format(\"Book\", \"True Author\", \"Predicted Book\", \"Predicted Author\"))\n",
    "    print(\"=\"*100)\n",
    "    for i in range(start_idx, end_idx):\n",
    "        book_label = df.loc[i, 'Book']\n",
    "        true_author = authors[i]  # Extract true author from the authors list\n",
    "        predicted_book = test_labels[i]  # Extract predicted book from the test_labels list\n",
    "        predicted_author = predictions[i]\n",
    "        print(\"{:<10} {:<30} {:<30} {:<30}\".format(book_label, true_author, predicted_book, predicted_author))\n",
    "\n",
    "def main():\n",
    "    classifiers = {\n",
    "        'Naive Bayes': MultinomialNB(),\n",
    "        'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "        'Support Vector Machine': SVC(probability=True),\n",
    "        'Decision Tree': DecisionTreeClassifier(),\n",
    "        'Random Forest': RandomForestClassifier(),\n",
    "        'Gradient Boosting': GradientBoostingClassifier()\n",
    "    }\n",
    "\n",
    "    for name, classifier in classifiers.items():\n",
    "        print(f\"Evaluating {name}...\")\n",
    "        vectorizer = TfidfVectorizer(max_features=1000)\n",
    "        trained_classifier, trained_vectorizer, authors = train_model(classifier, vectorizer)\n",
    "        evaluation_results = evaluate_model(trained_classifier, trained_vectorizer, authors)\n",
    "        print(f\"Results for {name}:\")\n",
    "        for metric, value in evaluation_results.items():\n",
    "            print(f\"{metric}: {value}\")\n",
    "\n",
    "        df = pd.read_csv('random_partitions.csv')\n",
    "        df['Processed_Partition'] = df['Partition'].apply(preprocess_text)\n",
    "        test_features = trained_vectorizer.transform(df['Processed_Partition']).toarray()\n",
    "        test_labels = df['Book']\n",
    "        predictions = trained_classifier.predict(test_features)\n",
    "        # visualize_results(test_labels, predictions)\n",
    "\n",
    "        # Print subset of results\n",
    "        start_idx = 10\n",
    "        end_idx = 20\n",
    "        print_results_subset(df, test_labels, predictions, authors, start_idx, end_idx)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88279d2366fe020547cde40dd65aa0e3aa662a6ec1f3ca12d88834876c85e1a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
