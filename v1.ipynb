{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Jainil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Jainil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\Jainil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Jainil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Naive Bayes...\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93momw-1.4\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('omw-1.4')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/omw-1.4\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Jainil/nltk_data'\n    - 'd:\\\\Anaconda\\\\nltk_data'\n    - 'd:\\\\Anaconda\\\\share\\\\nltk_data'\n    - 'd:\\\\Anaconda\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Jainil\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\nltk\\corpus\\util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mfind(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msubdir\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00mzip_name\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     85\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93momw-1.4\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('omw-1.4')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/omw-1.4.zip/omw-1.4/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Jainil/nltk_data'\n    - 'd:\\\\Anaconda\\\\nltk_data'\n    - 'd:\\\\Anaconda\\\\share\\\\nltk_data'\n    - 'd:\\\\Anaconda\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Jainil\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Jainil\\Desktop\\DSA_exercises\\v1.ipynb Cell 1\u001b[0m in \u001b[0;36m<cell line: 232>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Jainil/Desktop/DSA_exercises/v1.ipynb#W0sZmlsZQ%3D%3D?line=229'>230</a>\u001b[0m     plot_bias_variance(classifiers, results)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Jainil/Desktop/DSA_exercises/v1.ipynb#W0sZmlsZQ%3D%3D?line=231'>232</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/Jainil/Desktop/DSA_exercises/v1.ipynb#W0sZmlsZQ%3D%3D?line=232'>233</a>\u001b[0m     main()\n",
      "\u001b[1;32mc:\\Users\\Jainil\\Desktop\\DSA_exercises\\v1.ipynb Cell 1\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Jainil/Desktop/DSA_exercises/v1.ipynb#W0sZmlsZQ%3D%3D?line=209'>210</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEvaluating \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Jainil/Desktop/DSA_exercises/v1.ipynb#W0sZmlsZQ%3D%3D?line=210'>211</a>\u001b[0m vectorizer \u001b[39m=\u001b[39m TfidfVectorizer(max_features\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/Jainil/Desktop/DSA_exercises/v1.ipynb#W0sZmlsZQ%3D%3D?line=211'>212</a>\u001b[0m trained_classifier, trained_vectorizer, authors \u001b[39m=\u001b[39m train_model(classifier, vectorizer)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Jainil/Desktop/DSA_exercises/v1.ipynb#W0sZmlsZQ%3D%3D?line=212'>213</a>\u001b[0m evaluation_results \u001b[39m=\u001b[39m evaluate_model(trained_classifier, trained_vectorizer, authors)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Jainil/Desktop/DSA_exercises/v1.ipynb#W0sZmlsZQ%3D%3D?line=213'>214</a>\u001b[0m results[name] \u001b[39m=\u001b[39m evaluation_results\n",
      "\u001b[1;32mc:\\Users\\Jainil\\Desktop\\DSA_exercises\\v1.ipynb Cell 1\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(classifier, vectorizer)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Jainil/Desktop/DSA_exercises/v1.ipynb#W0sZmlsZQ%3D%3D?line=103'>104</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39mrandom_partitions.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Jainil/Desktop/DSA_exercises/v1.ipynb#W0sZmlsZQ%3D%3D?line=105'>106</a>\u001b[0m \u001b[39m# Preprocess the data\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/Jainil/Desktop/DSA_exercises/v1.ipynb#W0sZmlsZQ%3D%3D?line=106'>107</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mProcessed_Partition\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39;49m\u001b[39mPartition\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(preprocess_text)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Jainil/Desktop/DSA_exercises/v1.ipynb#W0sZmlsZQ%3D%3D?line=108'>109</a>\u001b[0m \u001b[39m# Feature extraction using TF-IDF\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Jainil/Desktop/DSA_exercises/v1.ipynb#W0sZmlsZQ%3D%3D?line=109'>110</a>\u001b[0m features \u001b[39m=\u001b[39m vectorizer\u001b[39m.\u001b[39mfit_transform(df[\u001b[39m'\u001b[39m\u001b[39mProcessed_Partition\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39mtoarray()\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\pandas\\core\\series.py:4433\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4323\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[0;32m   4324\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   4325\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4328\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   4329\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[0;32m   4330\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4331\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4332\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4431\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4432\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4433\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\pandas\\core\\apply.py:1082\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1078\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf, \u001b[39mstr\u001b[39m):\n\u001b[0;32m   1079\u001b[0m     \u001b[39m# if we are a string, try to dispatch\u001b[39;00m\n\u001b[0;32m   1080\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[1;32m-> 1082\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\pandas\\core\\apply.py:1137\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[0;32m   1132\u001b[0m         \u001b[39m# error: Argument 2 to \"map_infer\" has incompatible type\u001b[39;00m\n\u001b[0;32m   1133\u001b[0m         \u001b[39m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001b[39;00m\n\u001b[0;32m   1134\u001b[0m         \u001b[39m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m         \u001b[39m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m         \u001b[39m# \"Callable[[Any], Any]\"\u001b[39;00m\n\u001b[1;32m-> 1137\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[0;32m   1138\u001b[0m             values,\n\u001b[0;32m   1139\u001b[0m             f,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1140\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[0;32m   1141\u001b[0m         )\n\u001b[0;32m   1143\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1144\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1145\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1146\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2870\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Jainil\\Desktop\\DSA_exercises\\v1.ipynb Cell 1\u001b[0m in \u001b[0;36mpreprocess_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Jainil/Desktop/DSA_exercises/v1.ipynb#W0sZmlsZQ%3D%3D?line=85'>86</a>\u001b[0m \u001b[39m# Perform lemmatization\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Jainil/Desktop/DSA_exercises/v1.ipynb#W0sZmlsZQ%3D%3D?line=86'>87</a>\u001b[0m lemmatizer \u001b[39m=\u001b[39m WordNetLemmatizer()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Jainil/Desktop/DSA_exercises/v1.ipynb#W0sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m lemmatized_tokens \u001b[39m=\u001b[39m [lemmatizer\u001b[39m.\u001b[39mlemmatize(token, pos\u001b[39m=\u001b[39mget_wordnet_pos(token)) \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m stemmed_tokens]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Jainil/Desktop/DSA_exercises/v1.ipynb#W0sZmlsZQ%3D%3D?line=89'>90</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(lemmatized_tokens)\n",
      "\u001b[1;32mc:\\Users\\Jainil\\Desktop\\DSA_exercises\\v1.ipynb Cell 1\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Jainil/Desktop/DSA_exercises/v1.ipynb#W0sZmlsZQ%3D%3D?line=85'>86</a>\u001b[0m \u001b[39m# Perform lemmatization\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Jainil/Desktop/DSA_exercises/v1.ipynb#W0sZmlsZQ%3D%3D?line=86'>87</a>\u001b[0m lemmatizer \u001b[39m=\u001b[39m WordNetLemmatizer()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Jainil/Desktop/DSA_exercises/v1.ipynb#W0sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m lemmatized_tokens \u001b[39m=\u001b[39m [lemmatizer\u001b[39m.\u001b[39mlemmatize(token, pos\u001b[39m=\u001b[39mget_wordnet_pos(token)) \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m stemmed_tokens]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Jainil/Desktop/DSA_exercises/v1.ipynb#W0sZmlsZQ%3D%3D?line=89'>90</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(lemmatized_tokens)\n",
      "\u001b[1;32mc:\\Users\\Jainil\\Desktop\\DSA_exercises\\v1.ipynb Cell 1\u001b[0m in \u001b[0;36mget_wordnet_pos\u001b[1;34m(word)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Jainil/Desktop/DSA_exercises/v1.ipynb#W0sZmlsZQ%3D%3D?line=91'>92</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_wordnet_pos\u001b[39m(word):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Jainil/Desktop/DSA_exercises/v1.ipynb#W0sZmlsZQ%3D%3D?line=92'>93</a>\u001b[0m     \u001b[39m# Map POS tag to first character lemmatize() accepts\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Jainil/Desktop/DSA_exercises/v1.ipynb#W0sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m     tag \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mpos_tag([word])[\u001b[39m0\u001b[39m][\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mupper()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Jainil/Desktop/DSA_exercises/v1.ipynb#W0sZmlsZQ%3D%3D?line=94'>95</a>\u001b[0m     tag_dict \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mJ\u001b[39m\u001b[39m\"\u001b[39m: wordnet\u001b[39m.\u001b[39;49mADJ, \u001b[39m\"\u001b[39m\u001b[39mN\u001b[39m\u001b[39m\"\u001b[39m: wordnet\u001b[39m.\u001b[39mNOUN, \u001b[39m\"\u001b[39m\u001b[39mV\u001b[39m\u001b[39m\"\u001b[39m: wordnet\u001b[39m.\u001b[39mVERB, \u001b[39m\"\u001b[39m\u001b[39mR\u001b[39m\u001b[39m\"\u001b[39m: wordnet\u001b[39m.\u001b[39mADV}\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Jainil/Desktop/DSA_exercises/v1.ipynb#W0sZmlsZQ%3D%3D?line=95'>96</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m tag_dict\u001b[39m.\u001b[39mget(tag, wordnet\u001b[39m.\u001b[39mNOUN)\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\nltk\\corpus\\util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[39mif\u001b[39;00m attr \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__bases__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    119\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mLazyCorpusLoader object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m__bases__\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__load()\n\u001b[0;32m    122\u001b[0m \u001b[39m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[39m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, attr)\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\nltk\\corpus\\util.py:89\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     86\u001b[0m             \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[39m# Load the corpus.\u001b[39;00m\n\u001b[1;32m---> 89\u001b[0m corpus \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__reader_cls(root, \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__kwargs)\n\u001b[0;32m     91\u001b[0m \u001b[39m# This is where the magic happens!  Transform ourselves into\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[39m# the corpus by modifying our own __dict__ and __class__ to\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \u001b[39m# match that of the corpus.\u001b[39;00m\n\u001b[0;32m     95\u001b[0m args, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__kwargs\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py:1176\u001b[0m, in \u001b[0;36mWordNetCorpusReader.__init__\u001b[1;34m(self, root, omw_reader)\u001b[0m\n\u001b[0;32m   1172\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1173\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe multilingual functions are not available with this Wordnet version\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1174\u001b[0m     )\n\u001b[0;32m   1175\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1176\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprovenances \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49momw_prov()\n\u001b[0;32m   1178\u001b[0m \u001b[39m# A cache to store the wordnet data of multiple languages\u001b[39;00m\n\u001b[0;32m   1179\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lang_data \u001b[39m=\u001b[39m defaultdict(\u001b[39mlist\u001b[39m)\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py:1285\u001b[0m, in \u001b[0;36mWordNetCorpusReader.omw_prov\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1283\u001b[0m provdict \u001b[39m=\u001b[39m {}\n\u001b[0;32m   1284\u001b[0m provdict[\u001b[39m\"\u001b[39m\u001b[39meng\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1285\u001b[0m fileids \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_omw_reader\u001b[39m.\u001b[39;49mfileids()\n\u001b[0;32m   1286\u001b[0m \u001b[39mfor\u001b[39;00m fileid \u001b[39min\u001b[39;00m fileids:\n\u001b[0;32m   1287\u001b[0m     prov, langfile \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39msplit(fileid)\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\nltk\\corpus\\util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[39mif\u001b[39;00m attr \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__bases__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    119\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mLazyCorpusLoader object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m__bases__\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__load()\n\u001b[0;32m    122\u001b[0m \u001b[39m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[39m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, attr)\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\nltk\\corpus\\util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m             root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfind(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubdir\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mzip_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m             \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[39m# Load the corpus.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m corpus \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__reader_cls(root, \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__kwargs)\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\nltk\\corpus\\util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m         root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mfind(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msubdir\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__name\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     82\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     83\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93momw-1.4\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('omw-1.4')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/omw-1.4\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Jainil/nltk_data'\n    - 'd:\\\\Anaconda\\\\nltk_data'\n    - 'd:\\\\Anaconda\\\\share\\\\nltk_data'\n    - 'd:\\\\Anaconda\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Jainil\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import gutenberg\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from transformers import BertTokenizer, BertModel, XLNetTokenizer, XLNetModel\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "class GutenbergPartitioner:\n",
    "    def __init__(self):\n",
    "        # Select books from a specific genre\n",
    "        self.all_books = gutenberg.fileids()\n",
    "        self.selected_books = random.sample(self.all_books, 6)  # Choose 6 books randomly\n",
    "        self.book_data = [(book, gutenberg.raw(book)) for book in self.selected_books]\n",
    "\n",
    "    def get_author(self, book_text):\n",
    "        # Extract author information from book text\n",
    "        # This is a simplistic approach and may not work for all books\n",
    "        lines = book_text.split('\\n')\n",
    "        for line in lines:\n",
    "            if 'by' in line.lower():\n",
    "                author_index = line.lower().index('by')\n",
    "                author = line[author_index + 2:].strip()\n",
    "                # Remove any year like 1909] from author's name\n",
    "                author = ''.join([i for i in author if not i.isdigit() and i != ']'])\n",
    "                return author\n",
    "        return 'Unknown Author'\n",
    "\n",
    "    def split_into_partitions(self, num_partitions=200, partition_size=100):\n",
    "        partitions = []\n",
    "        for book, data in self.book_data:\n",
    "            author = self.get_author(data)\n",
    "            words = word_tokenize(data)\n",
    "            for _ in range(num_partitions):\n",
    "                start_index = random.randint(0, len(words) - partition_size)\n",
    "                partition = ' '.join(words[start_index:start_index + partition_size])\n",
    "                partitions.append((book, author, partition))\n",
    "\n",
    "        # Shuffle the order of partitions\n",
    "        random.shuffle(partitions)\n",
    "\n",
    "        # Label the partitions with alphabetical labels\n",
    "        labels = {book: chr(ord('a') + i) for i, book in enumerate(self.selected_books)}\n",
    "\n",
    "        # Add the labels to the partitions\n",
    "        labeled_partitions = [(labels[book], author, partition) for book, author, partition in partitions]\n",
    "        return labeled_partitions\n",
    "\n",
    "    def save_to_csv(self, labeled_partitions, output_filename='random_partitions.csv'):\n",
    "        # Serialize the data using Pandas\n",
    "        df = pd.DataFrame(labeled_partitions, columns=['Book', 'Author', 'Partition'])\n",
    "        df.to_csv(output_filename, index=False)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize, remove stopwords, and non-alphabetic characters\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    tokens = [word.lower() for word in tokens if word.isalpha() and word.lower() not in stopwords]\n",
    "\n",
    "    # Perform stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    # Perform lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token, pos=get_wordnet_pos(token)) for token in stemmed_tokens]\n",
    "\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    # Map POS tag to first character lemmatize() accepts\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def train_model(classifier, vectorizer):\n",
    "    partitioner = GutenbergPartitioner()\n",
    "    labeled_partitions = partitioner.split_into_partitions()\n",
    "    partitioner.save_to_csv(labeled_partitions)\n",
    "\n",
    "    # Load labeled partitions from CSV\n",
    "    df = pd.read_csv('random_partitions.csv')\n",
    "\n",
    "    # Preprocess the data\n",
    "    df['Processed_Partition'] = df['Partition'].apply(preprocess_text)\n",
    "\n",
    "    # Feature extraction using TF-IDF\n",
    "    features = vectorizer.fit_transform(df['Processed_Partition']).toarray()\n",
    "    print(features)\n",
    "    labels = df['Book']\n",
    "    authors = df['Author']  # Include authors as a separate column\n",
    "\n",
    "    # Train a machine learning model\n",
    "    classifier.fit(features, authors)  # Change labels to authors\n",
    "\n",
    "    return classifier, vectorizer, authors\n",
    "\n",
    "def evaluate_model(classifier, vectorizer, authors):\n",
    "    # Load labeled partitions from CSV\n",
    "    df = pd.read_csv('random_partitions.csv')\n",
    "\n",
    "    # Preprocess the data\n",
    "    df['Processed_Partition'] = df['Partition'].apply(preprocess_text)\n",
    "\n",
    "    # Feature extraction using TF-IDF\n",
    "    features = vectorizer.transform(df['Processed_Partition']).toarray()\n",
    "    labels = df['Book']\n",
    "\n",
    "    # Perform k-fold cross-validation\n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "    accuracy_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "    roc_auc_scores = []\n",
    "\n",
    "    for train_index, test_index in kf.split(features):\n",
    "        X_train, X_test = features[train_index], features[test_index]\n",
    "        y_train, y_test = authors[train_index], authors[test_index]  # Change labels to authors\n",
    "\n",
    "        classifier.fit(X_train, y_train)\n",
    "        y_pred = classifier.predict(X_test)\n",
    "\n",
    "        accuracy_scores.append(accuracy_score(y_test, y_pred))\n",
    "        precision_scores.append(precision_score(y_test, y_pred, average='weighted'))\n",
    "        recall_scores.append(recall_score(y_test, y_pred, average='weighted'))\n",
    "        f1_scores.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "        roc_auc_scores.append(roc_auc_score(y_test, classifier.predict_proba(X_test), multi_class='ovr'))\n",
    "\n",
    "    # Calculate bias and variance\n",
    "    bias = 1 - np.mean(accuracy_scores)\n",
    "    variance = np.var(accuracy_scores)\n",
    "\n",
    "    return {\n",
    "        'Bias': bias,\n",
    "        'Variance': variance,\n",
    "        'Accuracy': np.mean(accuracy_scores),\n",
    "        'Precision': np.mean(precision_scores),\n",
    "        'Recall': np.mean(recall_scores),\n",
    "        'F1-score': np.mean(f1_scores),\n",
    "        'ROC-AUC': np.mean(roc_auc_scores)\n",
    "    }\n",
    "\n",
    "def print_results_subset(df, test_labels, predictions, authors, start_idx, end_idx):\n",
    "    print(f\"\\nSubset of results:\")\n",
    "    print(\"{:<10} {:<30} {:<30} {:<30}\".format(\"Book\", \"True Author\", \"Predicted Book\", \"Predicted Author\"))\n",
    "    print(\"=\"*100)\n",
    "    for i in range(start_idx, end_idx):\n",
    "        book_label = df.loc[i, 'Book']\n",
    "        true_author = authors[i]  # Extract true author from the authors list\n",
    "        predicted_book = test_labels[i]  # Extract predicted book from the test_labels list\n",
    "        predicted_author = predictions[i]\n",
    "        print(\"{:<10} {:<30} {:<30} {:<30}\".format(book_label, true_author, predicted_book, predicted_author))\n",
    "\n",
    "def plot_bias_variance(classifiers, results):\n",
    "    classifier_names = list(classifiers.keys())\n",
    "    bias_values = [result['Bias'] for result in results.values()]\n",
    "    variance_values = [result['Variance'] for result in results.values()]\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(classifier_names, bias_values, marker='o', label='Bias')\n",
    "    plt.plot(classifier_names, variance_values, marker='o', label='Variance')\n",
    "    plt.xlabel('Classifier')\n",
    "    plt.ylabel('Bias/Variance')\n",
    "    plt.title('Bias and Variance of Classifiers')\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def main():\n",
    "    classifiers = {\n",
    "        'Naive Bayes': MultinomialNB(),\n",
    "        'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "        'Support Vector Machine': SVC(probability=True),\n",
    "        'Decision Tree': DecisionTreeClassifier(),\n",
    "        'Random Forest': RandomForestClassifier(),\n",
    "        'Gradient Boosting': GradientBoostingClassifier(),\n",
    "        'KNN': KNeighborsClassifier(),  # Add KNN Classifier\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for name, classifier in classifiers.items():\n",
    "        print(f\"Evaluating {name}...\")\n",
    "        vectorizer = TfidfVectorizer(max_features=1000)\n",
    "        trained_classifier, trained_vectorizer, authors = train_model(classifier, vectorizer)\n",
    "        evaluation_results = evaluate_model(trained_classifier, trained_vectorizer, authors)\n",
    "        results[name] = evaluation_results\n",
    "        print(f\"Results for {name}:\")\n",
    "        for metric, value in evaluation_results.items():\n",
    "            print(f\"{metric}: {value}\")\n",
    "\n",
    "        df = pd.read_csv('random_partitions.csv')\n",
    "        df['Processed_Partition'] = df['Partition'].apply(preprocess_text)\n",
    "        test_features = trained_vectorizer.transform(df['Processed_Partition']).toarray()\n",
    "        test_labels = df['Book']\n",
    "        predictions = trained_classifier.predict(test_features)\n",
    "\n",
    "        # Print subset of results\n",
    "        start_idx = 10\n",
    "        end_idx = 30\n",
    "        print_results_subset(df, test_labels, predictions, authors, start_idx, end_idx)\n",
    "\n",
    "    plot_bias_variance(classifiers, results)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88279d2366fe020547cde40dd65aa0e3aa662a6ec1f3ca12d88834876c85e1a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
