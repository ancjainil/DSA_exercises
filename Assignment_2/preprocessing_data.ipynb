{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/kish/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/kish/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/kish/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure NLTK resources are downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove unwanted characters and punctuation\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Remove white spaces\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Join words back into text\n",
    "    preprocessed_text = ' '.join(words)\n",
    "    \n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply lemmatization and tokenization\n",
    "def lemmatize_and_tokenize(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    lemmatized_text = ' '.join(lemmatized_tokens)\n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create random samples of 150 words\n",
    "def create_samples(text, num_samples=200, words_per_sample=150):\n",
    "    samples = []\n",
    "    words = text.split()\n",
    "    \n",
    "    # Calculate maximum number of samples that can be created\n",
    "    max_samples = len(words) // words_per_sample\n",
    "    \n",
    "    if max_samples < num_samples:\n",
    "        print(f\"Not enough words to create {num_samples} samples. Creating {max_samples} samples instead.\")\n",
    "        num_samples = max_samples\n",
    "    \n",
    "    # Create samples\n",
    "    for i in range(num_samples):\n",
    "        start_index = random.randint(0, len(words) - words_per_sample)\n",
    "        sample = ' '.join(words[start_index:start_index + words_per_sample])\n",
    "        samples.append(sample)\n",
    "    \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process books\n",
    "def process_books(book_paths):\n",
    "    df_data = []\n",
    "    \n",
    "    for book_path in book_paths:\n",
    "\n",
    "        book_name = os.path.basename(book_path)\n",
    "        # Preprocess the book\n",
    "        with open(book_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "            preprocessed_text = preprocess_text(text)\n",
    "        \n",
    "        # Save preprocessed book\n",
    "        preprocessed_file = os.path.splitext(book_path)[0] + \"_data.txt\"\n",
    "        with open(preprocessed_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(preprocessed_text)\n",
    "        \n",
    "        # Apply lemmatization and tokenization\n",
    "        lemmatized_text = lemmatize_and_tokenize(preprocessed_text)\n",
    "        \n",
    "        # Save lemmatized book\n",
    "        lemmatized_file = os.path.splitext(book_path)[0] + \"_lemmatized.txt\"\n",
    "        with open(lemmatized_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(lemmatized_text)\n",
    "        \n",
    "        # Create samples\n",
    "        print(book_name)\n",
    "        samples = create_samples(lemmatized_text)\n",
    "        \n",
    "        \n",
    "        # Add samples to DataFrame\n",
    "        for sample in samples:\n",
    "            df_data.append((sample, book_name))\n",
    "\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(df_data, columns=['Sample', 'Label'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Murder in the Gunroom.txt\n",
      "The Crime Club.txt\n",
      "The Devil Doctor.txt\n",
      "The House of Arrow.txt\n",
      "The Wrong Letter.txt\n",
      "Not enough words to create 200 samples. Creating 127 samples instead.\n",
      "The mystery of blue train.txt\n",
      "Time Crime.txt\n",
      "Not enough words to create 200 samples. Creating 127 samples instead.\n",
      "                                                 Sample  \\\n",
      "0     small case workbench armsbooks looked wall end...   \n",
      "1     said youd win piggybank seems mr fleming bough...   \n",
      "2     checkingupping done inside fleming household w...   \n",
      "3     pulled revolver pantsleg gave quick glance num...   \n",
      "4     got two stiff right pull bayonet closeup chest...   \n",
      "...                                                 ...   \n",
      "1249  general idea might able work location base lin...   \n",
      "1250  extremely ribald sign god watch u atarazola sa...   \n",
      "1251  verkan vall looked blank instant grinned long ...   \n",
      "1252  conveyer ball loaded sleepgas bomb rigged auto...   \n",
      "1253  terminal rocket ready ill need hypnomech khara...   \n",
      "\n",
      "                          Label  \n",
      "0     Murder in the Gunroom.txt  \n",
      "1     Murder in the Gunroom.txt  \n",
      "2     Murder in the Gunroom.txt  \n",
      "3     Murder in the Gunroom.txt  \n",
      "4     Murder in the Gunroom.txt  \n",
      "...                         ...  \n",
      "1249             Time Crime.txt  \n",
      "1250             Time Crime.txt  \n",
      "1251             Time Crime.txt  \n",
      "1252             Time Crime.txt  \n",
      "1253             Time Crime.txt  \n",
      "\n",
      "[1254 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Main function\n",
    "def main():\n",
    "    # Paths to input TXT files for seven books\n",
    "    book_paths = [\n",
    "        \"Dataset/Murder in the Gunroom.txt\",\n",
    "        \"Dataset/The Crime Club.txt\",\n",
    "        \"Dataset/The Devil Doctor.txt\",\n",
    "        \"Dataset/The House of Arrow.txt\",\n",
    "        \"Dataset/The Wrong Letter.txt\",\n",
    "        \"Dataset/The mystery of blue train.txt\",\n",
    "        \"Dataset/Time Crime.txt\"\n",
    "    ]\n",
    "    \n",
    "    # Process books\n",
    "    df = process_books(book_paths)\n",
    "    \n",
    "    # Display DataFrame\n",
    "    print(df)\n",
    "    \n",
    "    # Save DataFrame to CSV file\n",
    "    df.to_csv('book_samples.csv', index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
