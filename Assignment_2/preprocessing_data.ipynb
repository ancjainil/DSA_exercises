{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/kish/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/kish/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/kish/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure NLTK resources are downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove unwanted characters and punctuation\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Remove white spaces\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Join words back into text\n",
    "    preprocessed_text = ' '.join(words)\n",
    "    \n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply lemmatization and tokenization\n",
    "def lemmatize_and_tokenize(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    lemmatized_text = ' '.join(lemmatized_tokens)\n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create random samples of 150 words\n",
    "def create_samples(text, num_samples=200, words_per_sample=150):\n",
    "    samples = []\n",
    "    words = text.split()\n",
    "    \n",
    "    # Calculate maximum number of samples that can be created\n",
    "    max_samples = len(words) // words_per_sample\n",
    "    \n",
    "    if max_samples < num_samples:\n",
    "        print(f\"Not enough words to create {num_samples} samples. Creating {max_samples} samples instead.\")\n",
    "        num_samples = max_samples\n",
    "    \n",
    "    # Create samples\n",
    "    for i in range(num_samples):\n",
    "        start_index = random.randint(0, len(words) - words_per_sample)\n",
    "        sample = ' '.join(words[start_index:start_index + words_per_sample])\n",
    "        samples.append(sample)\n",
    "    \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process books\n",
    "def process_books(book_paths):\n",
    "    df_data = []\n",
    "    \n",
    "    for book_path in book_paths:\n",
    "\n",
    "        book_name = os.path.basename(book_path)\n",
    "        # Preprocess the book\n",
    "        with open(book_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "            preprocessed_text = preprocess_text(text)\n",
    "        \n",
    "        # Save preprocessed book\n",
    "        preprocessed_file = os.path.splitext(book_path)[0] + \"_data.txt\"\n",
    "        with open(preprocessed_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(preprocessed_text)\n",
    "        \n",
    "        # Apply lemmatization and tokenization\n",
    "        lemmatized_text = lemmatize_and_tokenize(preprocessed_text)\n",
    "        \n",
    "        # Save lemmatized book\n",
    "        lemmatized_file = os.path.splitext(book_path)[0] + \"_lemmatized.txt\"\n",
    "        with open(lemmatized_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(lemmatized_text)\n",
    "        \n",
    "        # Create samples\n",
    "        print(book_name)\n",
    "        samples = create_samples(lemmatized_text)\n",
    "        \n",
    "        \n",
    "        # Add samples to DataFrame\n",
    "        for sample in samples:\n",
    "            df_data.append((sample, book_name))\n",
    "\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(df_data, columns=['Sample', 'Book_Name'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Murder in the Gunroom.txt\n",
      "The Crime Club.txt\n",
      "The Devil Doctor.txt\n",
      "The House of Arrow.txt\n",
      "The Wrong Letter.txt\n",
      "Not enough words to create 200 samples. Creating 127 samples instead.\n",
      "The mystery of blue train.txt\n",
      "Time Crime.txt\n",
      "Not enough words to create 200 samples. Creating 127 samples instead.\n",
      "                                                 Sample  \\\n",
      "0     gresham couple day blowup late use rand said i...   \n",
      "1     case arm like cordwood still saying nothing ey...   \n",
      "2     asked gun river shop time umholtz rejuvenated ...   \n",
      "3     probably attributable even barbarous people sc...   \n",
      "4     monolithic fact officially attested indisputab...   \n",
      "...                                                 ...   \n",
      "1249  weapon want outland slave sort took sell big v...   \n",
      "1250  kharanda spoken possibility agent skordran kir...   \n",
      "1251  turned bowed two men white cloak slave noble l...   \n",
      "1252  modifier wrong place youre chief duplicate par...   \n",
      "1253  longlived race thin nose narrow bitter mouth l...   \n",
      "\n",
      "                      Book_Name  \n",
      "0     Murder in the Gunroom.txt  \n",
      "1     Murder in the Gunroom.txt  \n",
      "2     Murder in the Gunroom.txt  \n",
      "3     Murder in the Gunroom.txt  \n",
      "4     Murder in the Gunroom.txt  \n",
      "...                         ...  \n",
      "1249             Time Crime.txt  \n",
      "1250             Time Crime.txt  \n",
      "1251             Time Crime.txt  \n",
      "1252             Time Crime.txt  \n",
      "1253             Time Crime.txt  \n",
      "\n",
      "[1254 rows x 2 columns]\n",
      "['Murder in the Gunroom.txt' 'The Crime Club.txt' 'The Devil Doctor.txt'\n",
      " 'The House of Arrow.txt' 'The Wrong Letter.txt'\n",
      " 'The mystery of blue train.txt' 'Time Crime.txt']\n",
      "----\n",
      "Data processing completed and saved to processed_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Main function\n",
    "def main():\n",
    "    # Paths to input TXT files for seven books\n",
    "    book_paths = [\n",
    "        \"Dataset/Murder in the Gunroom.txt\",\n",
    "        \"Dataset/The Crime Club.txt\",\n",
    "        \"Dataset/The Devil Doctor.txt\",\n",
    "        \"Dataset/The House of Arrow.txt\",\n",
    "        \"Dataset/The Wrong Letter.txt\",\n",
    "        \"Dataset/The mystery of blue train.txt\",\n",
    "        \"Dataset/Time Crime.txt\"\n",
    "    ]\n",
    "    \n",
    "    # Process books\n",
    "    df = process_books(book_paths)\n",
    "    \n",
    "    # Display DataFrame\n",
    "    print(df)\n",
    "    print(df['Book_Name'].unique())\n",
    "    print(\"----\")\n",
    "    \n",
    "    # Save DataFrame to CSV file\n",
    "    df.to_csv('book_samples.csv', index=False)\n",
    "\n",
    "    # Read data from CSV file\n",
    "    data = pd.read_csv('book_samples.csv')\n",
    "\n",
    "    # Shuffle the data\n",
    "    data_shuffled = shuffle(data,random_state=42)\n",
    "\n",
    "    # Perform label encoding on the target variable\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_actual= label_encoder.fit_transform(data_shuffled['Book_Name'])\n",
    "    data_shuffled['Book_Name']=y_actual\n",
    "\n",
    "    # Write the processed data to a new CSV file\n",
    "    data_shuffled.to_csv('encoded_books_data.csv', index=False)\n",
    "\n",
    "    print(\"Data processing completed and saved to processed_data.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
